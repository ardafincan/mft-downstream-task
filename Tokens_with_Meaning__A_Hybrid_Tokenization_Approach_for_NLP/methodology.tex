\section{Methodology}
\label{sec:methodology}

We propose a hybrid tokenization framework that combines linguistic knowledge with statistical subword segmentation. This approach, the \textit{Morphology-First Tokenizer} (MFT), integrates rule-based morphological analysis with a structured dictionary of roots and affixes while incorporating BPE to handle out-of-vocabulary (OOV) terms. The objective is to create a tokenization system that accurate represents linguistic structures while maintaining computational efficiency.

\begin{figure}[H]
\centering
\resizebox{0.98\linewidth}{!}{%
\begin{tikzpicture}[
  font=\scriptsize,
  node distance=6mm and 8mm,
  every node/.style={align=center, transform shape},
  arrow/.style={thick,->,>=Stealth}
]

\tikzset{
  startstop/.style={ellipse, draw, fill=gray!10, minimum height=6mm, minimum width=15mm},
  process/.style={rounded corners, rectangle, draw, fill=orange!20, minimum height=7mm, text width=3.6cm},
  decision/.style={diamond, draw, fill=green!15, aspect=2.0, text width=3.1cm, inner sep=1pt},
  io/.style={trapezium, trapezium left angle=70, trapezium right angle=110, draw, fill=blue!10, minimum height=7mm, text width=3.6cm}
}

\node[startstop] (start) {Start};
\node[io, below=of start] (scan) {Scan input\\(segments: word / special)};

\node[decision, below=of scan] (special) {Special segment?};
\node[process, left=of special] (emit_special) {Emit special token};

\node[process, below=of special] (prep) {If capitalized: emit \texttt{<uppercase>}\\Lowercase word; normalize variants};

\node[decision, below=of prep] (root) {Root+suffix\\analysis succeeds?};
\node[process, left=of root] (emit_morph) {Emit root ID\\+ suffix IDs};
\node[process, right=of root] (emit_bpe) {BPE fallback\\(else \texttt{<unk>})};

\node[startstop, below=10mm of root] (end) {Next / End};

\draw[arrow] (start) -- (scan);
\draw[arrow] (scan) -- (special);
\draw[arrow] (special) -- node[above, pos=0.5] {Yes} (emit_special);
\draw[arrow] (emit_special) |- (end);
\draw[arrow] (special) -- node[right, pos=0.45] {No} (prep);

\draw[arrow] (prep) -- (root);
\draw[arrow] (root) -- node[above, pos=0.5] {Yes} (emit_morph);
\draw[arrow] (root) -- node[above, pos=0.5] {No} (emit_bpe);
\draw[arrow] (emit_morph) |- (end);
\draw[arrow] (emit_bpe) |- (end);

\end{tikzpicture}%
}
\caption{Algorithmic flow of the MFT tokenization pipeline.}
\label{fig:flowchart}
\end{figure}

We provide a Python reference implementation of the tokenizer and release the lexical resources (root and affix inventories) and decoder rules used in our experiments.

\textbf{Dictionary Construction.} The core of MFT is a dual-dictionary system designed to cover the productive morphology of Turkish.

\textbf{Root Dictionary.} The root dictionary is constructed from high-frequency words extracted from large-scale Turkish corpora, comprising approximately 22,000 roots. To address the challenge of phonological alternation, we employ a normalization strategy where surface variants map to a single canonical root ID. For example, consonant alternation causes \textit{kitap} (book) and \textit{kitabı} (its book) to share the same root ID despite the $p \to b$ softening; vowel hiatus maps \textit{oyna} (play) and \textit{oynuyor} (playing, where `a' drops) to a unified token; and haplology treats \textit{alın} (forehead) and \textit{alnı} (his forehead) identically. We also explicitly tokenize frequent compound words (e.g., \textit{akarsu} `stream', \textit{çamaşırhane} `laundromat') as single units to prevent erroneous splitting.

\textbf{Affix Dictionary.} The affix inventory consists of 177 suffix surface forms consolidated into 72 abstract affix IDs (covering grammatical morphemes such as case markers, tense suffixes, and derivational endings). Similar to roots, we merge allomorphs that serve identical grammatical functions into shared IDs. For instance, the plural suffix \textit{-ler} and its harmonic variant \textit{-lar} are assigned a single token ID (e.g., \textsc{PL}), as are the ablative variants \textit{-den, -dan, -ten, -tan}. This abstraction reduces vocabulary redundancy while preserving the morphosyntactic signal.

\textbf{Input Normalization and Special Tokens.} To ensure robustness across diverse text inputs, we implement strict normalization rules. For case handling, we introduce an \texttt{<uppercase>} token to mark capitalized words, allowing the model to process \textit{Kitap} and \textit{kitap} using the same root embedding and effectively halving the number of required surface forms. For CamelCase splitting, technical terms and code-switching often introduce CamelCase (e.g., `HTTPServer), which we split into constituent parts (`HTTP, `Server) before tokenization to improve subword coverage. Word boundaries are marked by including a leading space in root tokens, ensuring that tokenization is lossless and reversible without a dedicated whitespace token.

\textbf{Encoding Algorithm.} The encoding process (Algorithm \ref{alg:encoding}) follows a ``longest-prefix match'' strategy. For each word, the tokenizer first attempts to identify a valid root from the dictionary. If a root is found, it greedily matches the longest chain of valid suffixes.

\begin{algorithm}
\caption{Morphology-First Tokenization Pipeline}
\label{alg:encoding}
\begin{algorithmic}[1]
\State \textbf{Input:} Raw text string $S$
\State \textbf{Output:} Sequence of Token IDs $T$
\State $S \gets \text{Preprocess}(S)$ \Comment{Insert spaces, split CamelCase}
\For{each word $w$ in $S$}
    \If{$w$ is Space or Punctuation}
        \State $T.\text{append}(\text{GetSpecialID}(w))$
        \State \textbf{continue}
    \EndIf
    \If{$w$ is Capitalized}
        \State $T.\text{append}(\text{ID}_{\text{uppercase}})$
        \State $w \gets w.\text{lower}()$
    \EndIf
    \State $root, suffixes \gets \text{MorphAnalyze}(w)$ \Comment{Greedy Dictionary Search}
    \If{$root \neq \text{None}$}
        \State $T.\text{append}(root.\text{id})$
        \For{$s$ in $suffixes$}
            \State $T.\text{append}(s.\text{id})$
        \EndFor
    \Else
        \State $subwords \gets \text{BPE}(w)$ \Comment{Fallback}
        \State $T.\text{extend}(subwords)$
    \EndIf
\EndFor
\State \textbf{Return} $T$
\end{algorithmic}
\end{algorithm}

If the morphological analyzer fails to cover the word (i.e., no valid root+suffix combination is found), the system falls back to a BPE model. This ensures that the tokenizer remains open-vocabulary and can handle foreign entities or neologisms. The BPE model is trained on a version of the corpus where known morphological segments are masked, focusing its vocabulary (12,696 tokens) on residual stems and subwords.

\textbf{Example.} Consider the validation sentence: ``\textit{Kalktığımızda hep birlikte yürüdük.}'' (When we stood up, we walked together.)

\glossex{\texttt{<upper>} kalk-tığ-ımız-da \_ hep \_ birlik-te \_ yürü-dü-k .}
{CAPS stand-NMZ-POSS.1PL-LOC \_ always \_ unity-LOC \_ walk-PST-1PL .}
{When we stood up, we walked together.}
{Kalktığımızda hep birlikte yürüdük.}

Here, the tokenizer correctly identifies the root \textit{kalk} (stand) and segments the complex nominalization chain \textit{-tık-ımız-da} (represented by phonologically normalized abstract suffixes).

\textbf{Decoding Algorithm.} Decoding in MFT is non-trivial compared to standard subword tokenizers. Simple concatenation is insufficient due to the normalization of affixes. The decoder (Algorithm~\ref{alg:decoding}) applies phonological rules to reconstruct the correct surface form by selecting the appropriate allomorphic variant for each suffix based on context.

\begin{algorithm}[H]
\caption{MFT Decoding Pipeline}
\label{alg:decoding}
\begin{algorithmic}[1]
\State \textbf{Input:} Sequence of Token IDs $T$
\State \textbf{Output:} Reconstructed text string $S$
\State $\text{parts} \gets []$; $i \gets 0$
\While{$i < |T|$}
    \State $id \gets T[i]$
    \If{$id = \text{ID}_{\text{uppercase}}$}
        \State $\text{parts}.\text{append}(\text{TurkishCapitalize}(\text{Lookup}(T[i+1])))$
        \State $i \gets i + 2$; \textbf{continue}
    \EndIf
    \State $\text{candidates} \gets \text{ReverseLookup}(id)$ \Comment{Surface form variants}
    \If{$|\text{candidates}| > 1$}
        \State $\text{ctx} \gets \text{GetVowelContext}(\text{parts})$ \Comment{Look back for vowel}
        \State $\text{surface} \gets \text{ApplyPhonology}(id, \text{ctx}, T, i)$
    \Else
        \State $\text{surface} \gets \text{candidates}[0]$
    \EndIf
    \State $\text{parts}.\text{append}(\text{surface})$; $i \gets i + 1$
\EndWhile
\State \textbf{Return} $\text{parts}.\text{join}(\text{``''})$
\end{algorithmic}
\end{algorithm}

The \texttt{ApplyPhonology} function implements five Turkish morphophonological rules:

\begin{enumerate}
    \item \textbf{Vowel harmony (front/back):} Suffix vowels match the frontness of the last vowel (e.g., \textit{-ler} after \textit{ev} but \textit{-lar} after \textit{çocuk}; front vowels: e,i,ö,ü; back vowels: a,ı,o,u)
    \item \textbf{Consonant assimilation:} Initial $d \to t$ after voiceless consonants (e.g., \textit{-da} $\to$ \textit{-ta} after \textit{kitap}; voiceless: f,s,t,k,ç,ş,h,p)
    \item \textbf{Consonant softening:} Final $p \to b$, $k \to ğ$, $t \to d$, $ç \to c$ before vowel-initial suffixes (e.g., \textit{kitap} $\to$ \textit{kitab-ı})
    \item \textbf{Vowel narrowing:} Stem-final $e \to i$, $a \to ı$ before progressive \textit{-yor} (e.g., \textit{de-} $\to$ \textit{di-yor}, \textit{başla-} $\to$ \textit{başlı-yor})
    \item \textbf{Buffer consonant insertion:} $y$/$n$/$s$ inserted between vowels (e.g., \textit{okuma} + ACC $\to$ \textit{okuma-y-ı})
\end{enumerate}

\textbf{Example.} \glossex{\texttt{<upper>} kitap \_ okuma-yı \_ sev-iyor-um .}
{CAPS book \_ reading-ACC \_ love-PROG-1SG .}
{I like reading books.}
{Kitap okumayı seviyorum.}

In this example, the abstract accusative suffix (represented as \textsc{ACC} in the vocabulary) is realized as \textit{-yı} after \textit{okuma} due to buffer consonant insertion (rule 5) and vowel harmony (rule 1).

\textbf{Roundtrip Reconstruction Evaluation.} To validate the reconstruction property of the decoder, we evaluate word-level roundtrip accuracy on 66,547 words from the Cosmos corpus. Given an input word $w$, we compute $\hat{w} = \text{decode}(\text{encode}(w))$ and measure exact-match accuracy. The decoder achieves \textbf{99.48\% exact-match accuracy} (66,200/66,547 words). The remaining 0.52\% failures arise from inherent ambiguity in Turkish phonology: vowel alternation patterns in complex verb forms (e.g., \textit{tetkiki} $\to$ \textit{tetkiği}) where multiple surface realizations are linguistically valid for the same morpheme sequence. This is not a limitation of the tokenizer but rather reflects the intrinsic many-to-one mapping in Turkish morphophonology, where the same abstract morpheme can surface differently depending on context.

Crucially, this near-lossless reconstruction property enables MFT to be used across all transformer architectures: \textbf{encoder-only models} (e.g., BERT-style embeddings), \textbf{encoder-decoder models} (e.g., translation, summarization), and \textbf{decoder-only models} (e.g., GPT-style generation). For encoder-only tasks, exact reconstruction is not required since the model operates on embeddings rather than regenerating text. For generative tasks, the 99.48\% accuracy ensures that the vast majority of outputs are orthographically correct, with the rare exceptions being phonologically valid Turkish variants.

\textbf{Tokenization Efficiency.} We benchmark tokenization speed and token density on 1,000 Turkish text samples from our corpus. Table~\ref{tab:tokenization_efficiency} compares MFT against three Turkish-trained baseline tokenizers under matched vocabulary size (32,768).

\begin{table}[H]
\centering
\caption{Tokenization efficiency comparison (1,000 texts, 653K words).}
\label{tab:tokenization_efficiency}
\begin{tabular}{lrrrr}
\toprule
\textbf{Tokenizer} & \textbf{Time (ms)} & \textbf{Tokens} & \textbf{Tok/Word} & \textbf{Tok/Char} \\
\midrule
MFT & 1,935 & 1,899,670 & \textbf{2.91} & 0.356 \\
Tabi & 1,544 & 1,298,725 & 1.99 & 0.244 \\
Mursit & 1,655 & 1,187,418 & 1.82 & 0.223 \\
Cosmos & 1,620 & 1,186,834 & 1.82 & 0.223 \\
\bottomrule
\end{tabular}
\end{table}

MFT produces approximately 1.5$\times$ more tokens than BPE-based baselines because it segments at morpheme boundaries (e.g., \textit{kitaplarımızdan} $\to$ 4 tokens: \textit{kitap + lar + ımız + dan}) rather than optimizing for compression. While this increases sequence length, the morpheme-aligned representations yield improved downstream performance: +16.8~pp on STSb-TR correlation and +7.8~pp on retrieval tasks compared to the strongest baseline. The trade-off favors semantic quality over sequence efficiency—consistent with our hypothesis that morphological alignment improves sample efficiency during embedding training.

